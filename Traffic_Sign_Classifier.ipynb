{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "In this notebook, a model to classify traffic signs is designed based on LeNet model. The data set for the project is based on the [German Traffic Sign Dataset](https://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset). \n",
    "\n",
    "Please refer to the [result file](./Traffic_Sign_Classifier.html) for the final results of the project.\n",
    "The [project writeup](./README.md) file summarizes my approach towards data analysis, data augmentation techniques and last by not the least, the project results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration of global parameters\n",
    "Adapt the below code block to configure the data sets. The hyperparemeters for training are not listed here. It has to be adjusted in the respective chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this flag to use a archived file instead of generating fresh images\n",
    "use_augmented_datafile = False\n",
    "if use_augmented_datafile :\n",
    "    #data file which contains additional images generated using keras.ImageDataGenerator\n",
    "    training_file = '..//data/augmented_data.pbz2'\n",
    "else:\n",
    "    #data set containing the training images provided by German Traffic Sign Dataset\n",
    "    training_file = '..//data/train.p'\n",
    "# data set containing the validation images provided by German Traffic Sign Dataset \n",
    "validation_file= '..//data/valid.p'\n",
    "# data files containing the test images provided by German Traffic Sign Dataset\n",
    "testing_file = '..//data/test.p'\n",
    "# file which has the mapping of class ids and class labels\n",
    "label_file = './signnames.csv'\n",
    "# path where the trained model will be archived\n",
    "model_path = './models/traffic_sign_v2'\n",
    "# set this flag to true when new training has to be performed\n",
    "perform_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import bz2\n",
    "import _pickle as cPickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_pickle_data():\n",
    "    # Load pickled data\n",
    "    if use_augmented_datafile :\n",
    "        print('Uncompressing data file. This may take a while ...')\n",
    "        train = bz2.BZ2File(training_file, 'rb')\n",
    "        dist_pickle = cPickle.load(train)\n",
    "        print('Loading of data file is complete.')\n",
    "        # Unpickle the augmented training and validation \n",
    "        # data from the compressed pickle file\n",
    "        X_train = dist_pickle['X_train'].astype('uint8') \n",
    "        y_train = dist_pickle['y_train'].astype('uint8') \n",
    "    else:\n",
    "        with open(training_file, mode='rb') as f:\n",
    "            train = pickle.load(f)\n",
    "        # Unpickle the training data from the pickle file\n",
    "        X_train, y_train = train['features'], train['labels']\n",
    "  \n",
    "    with open(validation_file, mode='rb') as f:\n",
    "        valid = pickle.load(f)\n",
    "    with open(testing_file, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    # Unpickle the validation and test data from the pickle file    \n",
    "    X_valid, y_valid = valid['features'], valid['labels']\n",
    "    X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "    # Load the class-ids and the  corresponding texts from the csv file\n",
    "    df =  pd.read_csv('./signnames.csv') \n",
    "    label_texts = list(df['SignName']) \n",
    "    label_ids = list(df['ClassId'])\n",
    "    #since label texts are already sorted in the CSV file, the signnames array\n",
    "    #index can be directly used for 'mapping'\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, label_texts, label_ids\n",
    "\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test, label_texts, label_ids = load_pickle_data()\n",
    "print ('Datasets loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Summary of the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_data(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    \n",
    "    # Number of training examples\n",
    "    n_train = X_train.shape[0]\n",
    "    \n",
    "    # Number of validation examples\n",
    "    n_validation = X_valid.shape[0]\n",
    "    \n",
    "    # Number of testing examples.\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    # The shape of an traffic sign image\n",
    "    # The Input 'features' is a 4D array containing raw pixel data of the traffic sign \n",
    "    # images, (num examples, width, height, channels).\n",
    "    image_shape = X_train[0].shape\n",
    "    \n",
    "    # unique classes/labels there are in the dataset.\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    print(\"Number of training data =\", n_train)\n",
    "    print(\"Number of validation data =\", n_validation)\n",
    "    print(\"Number of testing data =\", n_test)\n",
    "    print(\"Image data shape =\", image_shape)\n",
    "    print(\"Number of classes =\", n_classes)\n",
    "\n",
    "    return n_train, n_validation , n_test, image_shape, n_classes\n",
    "\n",
    "#prints out the count of data in the training, validation and test data sets\n",
    "n_train, n_validation , n_test, image_shape, n_classes = summarize_data(X_train, y_train, \\\n",
    "                                                                        X_valid, y_valid, \\\n",
    "                                                                        X_test, y_test)\n",
    "# check if the count of inputs match with the corresponding labels\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_valid) == len(y_valid))\n",
    "assert(len(X_test) == len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we load a sample set of images from the database. Later, the distribution of classes in the training, validation and test sets are plotted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects <image_count> number of random images from the given dataset (X_train, y_train) \n",
    "# and displays them in a 6 x 7 grid. The title of each image will be of the format:\n",
    "# <Class_ID>-μ=<mean of image> σ=<variance of image>\n",
    "def random_plot(X_train, y_train, image_count=42):\n",
    "    images_per_row = 6\n",
    "    #count more than the mulitples of 'images_per_row' will be ignored\n",
    "    rows = int(image_count/images_per_row)\n",
    "    length_train_data = len(X_train)\n",
    "    \n",
    "    #prepare a random 'index' list from the training dataset\n",
    "    indexs = random.sample(range(length_train_data), image_count)\n",
    "    #plot the randomly selected signs \n",
    "    fig, axes = plt.subplots(rows,images_per_row, figsize=(16,12))\n",
    "    for ax , index in zip( axes.flatten(), indexs) :\n",
    "        ax.imshow(X_train[index].astype('uint8'))\n",
    "        #picks the labels from the 'master' labels loaded from signposts.csv\n",
    "        title = '[{}]μ={:.2f} σ={:.2f}'.format(y_train[index], X_train[index].mean(), X_train[index].std())\n",
    "        ax.set_title(title , fontsize=8)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        \n",
    "random_plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots the distribution of data as a Historgram plot.\n",
    "# The bar will be colored red if the frequency of a class is less than 60% of the max\n",
    "# else, it will be colored green\n",
    "def plot_class_distribution(labels, data, title ):\n",
    "    count_classes = np.zeros(n_classes)\n",
    "    colors = np.array(n_classes)\n",
    "    \n",
    "    for i in range (n_classes):\n",
    "        count = np.where(data == i )\n",
    "        count_classes[i] = len(count[0])\n",
    "    colors = ['g' if count_classes[i] > count_classes.max()*0.60 else 'r' for i  in range(n_classes)]\n",
    "    #plot a bar chart to show the frequency of the signnames in the training set\n",
    "    fig = plt.figure(figsize=(10,12))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Data count', fontsize=16)\n",
    "    ax.set_ylabel(' ', fontsize=9)\n",
    "    plt.barh( labels, count_classes, height = 0.8 , align = 'center', color = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the distribution in the Training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare labels in the format LABEL[ID]\n",
    "labels = [ label + '[' + str(id)+ ']' for label, id in zip(label_texts,label_ids)]\n",
    "plot_class_distribution(labels, y_train, 'Training data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the distribution in the Validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution(labels, y_valid, 'Validation data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the distribution in the Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution(labels, y_test, 'Test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Though there is an imbalance in the test data set, i am leaving it 'alone' since this data is not used for training purposes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Summary:** The histogram (when generated without the augmented datasets) shows that the distribution of the classes are unbalanced.<br> **Augmentation** techniques needs to be applied (Further details in pre-processing chapter) to those classes which are under-represented. Augmentation will be applied **only for training and validation data sets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) is taken as a starting point. \n",
    "\n",
    "Before building the model, the images in the Training and validation test should be pre-processed so that the neural network can work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram data from the previous chapter, it is clear that the training samples are under-represented. For example, the class \"Speed Limit 20km/h\" is represented by 180 samples, while the class \"Speed Limit 20km/h\" is represented by more than 2000 samples. Augmentation of training data with pseudo images will balance out this under-representation.\n",
    "**NOTE** - The Data Augmentation related code will be run only if the flag is `use_augmented_datafile` is set to `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "#generates pseudo images based on the training set\n",
    "def augment_pseudo_data(X_train, y_train, target_class_size = 2500):\n",
    "    #datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1,\n",
    "    #                                     height_shift_range=0.1, shear_range=0.1,\n",
    "    #                                     zoom_range=0.1,fill_mode='nearest')\n",
    "    datagen = ImageDataGenerator(rotation_range=5, width_shift_range=0.1,\n",
    "                                         height_shift_range=0.1,\n",
    "                                         zoom_range=0.1,fill_mode='nearest')\n",
    "    #Initialise empty arrays to hold the pseudo images and labels\n",
    "    X_pseudo = np.empty((0,32,32,3), dtype='uint8')\n",
    "    y_pseudo = np.empty(0 , dtype='uint8')\n",
    "    # iterate through all the labels\n",
    "    for i in tqdm(range(n_classes)):\n",
    "        # \"Real\" Images refer to those images in the German Traffic Sign Database\n",
    "        # filter out the current set of \"real\" images and labels\n",
    "        X_real = X_train[y_train == i]\n",
    "        y_real = np.repeat(i, X_real.shape[0])\n",
    "        # define a container for holding the pseudo images and labels\n",
    "        # \"Pseudo\" refers to the immages and labels created by ImageDataGenerator.\n",
    "        _X_pseudo = np.empty((0,32,32,3), dtype='uint8')\n",
    "        _y_pseudo = np.empty(0, dtype='uint8')\n",
    "        # Pass on the real images belonging to the respective class\n",
    "        # keras.ImageDataGenerator does all the hard work and returns the pseudo images\n",
    "        # and labels in x and y respectively\n",
    "        for x,y in datagen.flow(X_real, y_real, batch_size=len(y_real), seed=i):\n",
    "            #append the generated images and labels to \"Pseudo\" array\n",
    "            _X_pseudo = np.concatenate((_X_pseudo, x), axis = 0)\n",
    "            _y_pseudo = np.concatenate((_y_pseudo, y), axis = 0)\n",
    "            # copy the pseudo images and labels to the master array\n",
    "            if (X_real.shape[0] + _X_pseudo.shape[0] > target_class_size):\n",
    "                # Break when the total (Real+Pseudo) counts of images per class reaches\n",
    "                # the target limit \n",
    "                extra_images = target_class_size - _X_pseudo.shape[0] - X_real.shape[0]\n",
    "                _X_pseudo = _X_pseudo[:extra_images]\n",
    "                _y_pseudo = _y_pseudo[:extra_images]\n",
    "                break\n",
    "        # add the pseudo image for the current class\n",
    "        X_pseudo = np.concatenate((X_pseudo, _X_pseudo), axis = 0)\n",
    "        y_pseudo = np.concatenate((y_pseudo, _y_pseudo), axis = 0)\n",
    "    #return the complete array\n",
    "    return X_pseudo , y_pseudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the augment function on the training data.\n",
    "if not use_augmented_datafile:\n",
    "    X_pseudo, y_pseudo = augment_pseudo_data(X_train, y_train, target_class_size=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Call the random plot function to verify the augmented data\n",
    "if not use_augmented_datafile:\n",
    "    random_plot(X_pseudo, y_pseudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge pseudo data into the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print ('Training samples before augmentation : ' + str(X_train.shape[0]))\n",
    "print ('Validation samples before augmentation : ' + str(X_valid.shape[0]))\n",
    "\n",
    "X_train = np.concatenate((X_train, X_pseudo), axis=0)\n",
    "y_train = np.concatenate((y_train, y_pseudo), axis=0)\n",
    "\n",
    "print ('Training samples after augmentation : ' + str(X_train.shape[0]))\n",
    "print ('Validation samples after augmentation : ' + str(X_valid.shape[0]))\n",
    "\n",
    "print ('Splitting the training and validation data sets after augmentation  ... ')\n",
    "\n",
    "# Random state with an integer will produce the same results across different calls\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \\\n",
    "                                                      test_size=0.2, \\\n",
    "                                                      shuffle=True, random_state=0)\n",
    "\n",
    "print ('Training samples after splitting : ' + str(X_train.shape[0]))\n",
    "print ('Validation samples after splitting : ' + str(X_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification of the distribution of the labels after augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the labels for chart\n",
    "if not use_augmented_datafile:\n",
    "    # Format the labels as <Classname>[ClassID]\n",
    "    labels = [ label + '[' + str(id)+ ']' for label, id in zip(label_texts,label_ids)]\n",
    "    plot_class_distribution(labels, y_train, 'Training data (after Augmentation)')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows that the distribution of classes within the **training and validation dataset is well balanced**. Now it is time for normalizing the images.\n",
    "\n",
    "But, before that, the pseudo data is saved as a compressed zip file. This code will run only if the flag is `use_augmented_datafile` is set to `True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serializing the augmented training data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the development process, the pesudo files along with the real training data are saved into a pickle file. The classic pickling using pickle class resulted in a pickle file which was more than 1.05 GB. With the help of the [following article](https://medium.com/better-programming/load-fast-load-big-with-compressed-pickles-5f311584507e), i could save more than 80% disk space at the cost of excuetion time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_augmented_data(filepath, X_train, y_train ):\n",
    "    with bz2.BZ2File(filepath, 'w') as f:#\n",
    "        data_pickle = {}\n",
    "        print (\"Saving object with {} items \".format(str(X_train.shape)))\n",
    "        data_pickle['X_train'] = X_train\n",
    "        data_pickle['y_train'] = y_train\n",
    "        cPickle.dump(data_pickle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_augmented_datafile:\n",
    "    save_augmented_data( training_file , X_train, y_train)\n",
    "    print ('Data saved at ' + training_file )\n",
    "else:\n",
    "    print ('Augmentation not configured. Set flag use_augmented_datafile to True.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After augmentation of pseudo images, the next pre-processing steps are applied to the augmented training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic functions for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixels of the images \n",
    "def normalize(images):\n",
    "    result = np.zeros((len(images), 32, 32, 3), np.float64)\n",
    "    for index, image in enumerate(images):\n",
    "        #retain the image depth to 3\n",
    "        gray = cv2.cvtColor(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "        result[index] = np.array((gray - np.min(gray)) / (np.max(gray) - np.min(gray)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire training set images are normalized so that image pixels are centered around the mean and standard deviation of the respective image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train)\n",
    "X_valid = normalize(X_valid)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "print ('Training samples after normalization : ' + str(X_train.shape[0]))\n",
    "print ('Validation samples before normalization : ' + str(X_valid.shape[0]))\n",
    "print ('Test samples before normalization : ' + str(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification of the pre-processing of the augmented dataset \n",
    "\n",
    "#prepare a random 'index' list from the training dataset\n",
    "indexs = random.sample(range(X_train.shape[0]), 42)\n",
    "#plot the randomly selected signs \n",
    "fig, axes = plt.subplots(7,6, figsize=(18,18))\n",
    "for ax , index in zip( axes.flatten(), indexs) :\n",
    "    ax.imshow(X_train[index])\n",
    "    #picks the labels from the 'master' labels loaded from signposts.csv\n",
    "    title = '[{}]μ={:.2f} σ={:.2f}'.format(y_train[index], \n",
    "                                           X_train[index].mean(), \n",
    "                                           X_train[index].std())\n",
    "    ax.set_title(title)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Summary:** The above plot shows that the normalization process had led to a mean value closer to 0 and equal varaince across the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is implemented based on the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. After the pre-processing step, the images still retain the color depth of 3. Hence C is 3 in this case.\n",
    "\n",
    "### Architecture \n",
    "**Layer 1: Convolutional.** The starting point is a convolutional layer with 16 filters (filter-depth), each filter sliding (more precisely convolving) a patch of 5x5x3 kernel across the 32x32x3 input image. This layer uses valid padding with a stride of 1. The dimensions of the output of this layer will therefore be 28x28x16, which is the so-called feature map that predicts the class to which each feature belongs. Since the parameters are shared across this spatial arrangement, the number of parameters will be filter size (5x5x3) * 16 filter = 1210 Weights + 16 Biases = 1216 parameters\n",
    "\n",
    "**Activation:** Next, the output of the convolutional layer is activated with a RELU activation function, which adds non-linearity to the neural network\n",
    "\n",
    "**Pooling:** Next, the output from the activation layer is pooled using the max-pooling method with the most commomly used kernel size of 2x2 and stride size of 2x2 which gives a pooling output of 14x14x16. This pooling layer helps to reduce the the number of parameters, memory footprint and amount of computation in the network. In addition, it helps to control overfitting. The pooling does not introduces any new parameters since it computes a fixed function of the input.\n",
    "\n",
    "**Layer 2: Convolutional** The network then runs through another set of convolutional layer with 64 filters and a patch size of 5x5, RELU activation and a max-pooling layer giving an output of 5x5x64. The number of parameters in this layer will be filter size (5x5x16) * 64 filter = 25600 Weights + 64 biases = 25664 parameters\n",
    "\n",
    "**Flatten.** The output is flattened into a vector. The length of the vector is 5x5x64 = 1600. \n",
    "\n",
    "**Layer 3: Fully Connected.** The vector is passed on to a fully connected layer with a width of 240, each representing a probability that a certain feature belongs to a label. The fully connected layer goes through its own backpropagation process to determine the most accurate weights. Each neuron receives weights that prioritize the most appropriate label.In simple words, while the Convolution layer extracts the feature maps, the fully connected layers classifies those extracted features. (Wx + b)\n",
    "\n",
    "**Activation.** The above FC layer is followed by a RELU activation function g(Wx + b). In the previous FC layer, the function Wx + b results in a linear projection from the input to the output. RELU function g(Wx +b) introduces non-linearity to the network again.\n",
    "\n",
    "**Dropout.** A dropout layer with 60% probability is added after the RELU activation layer. This means that there is a 60% change that the output of a given neuron will be forced to 0. This avoid over-fitting of the network. This layer was added after noticing that the validation accuracy was not improving after 3 or 4 epochs. There was progressive improvment in validation accuracy after including the drop-outs.\n",
    "\n",
    "**Layer 4: Fully Connected.** The output of the dropout layer to connected to the second fully connected layers with a width of  84 outputs, each representing a probability that a certain feature belongs to a label\n",
    "\n",
    "**Activation.** As before, to remove the non-linearity, a RELU activation layer is added.\n",
    "\n",
    "**Dropout.** As layer 4, a dropout layer with 60% probability is added after the RELU activation layer.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This final layer returns the un-normaled predictions (logits) of the model, with a width of 43, each representing the logit values for a particular class.\n",
    "\n",
    "### Output\n",
    "Return the logits from the layer 5. A softmax function should be run on this value to map the result between 0 to 1.\n",
    "\n",
    "### Summary\n",
    "|Layer (type)|Output Shape |Param #|\n",
    "|:- |:-- |:-:|\n",
    "|**Layer1**-Convolution(Conv2D)|(None, 28, 28, 16)|1216|\n",
    "|RELU Activation|(None, 28, 28, 16)|0|\n",
    "|Max pooling|(None, 14, 14, 16)|0|\n",
    "|**Layer2**-Convolution(Conv2D)|(None, 10, 10, 64)|25664|\n",
    "|RELU Activation|(None, 10, 10, 64)|0|\n",
    "|Max pooling|(None, 5, 5, 64)|0|\n",
    "|Flatten|(None, 1600)| 0|\n",
    "|**Fully Connnected Layer1**|(None, 240)|384240|\n",
    "|RELU Activation|(None, 240)|0|\n",
    "|Dropout|(None, 240)|0|\n",
    "|**Fully Connnected Layer2**|(None, 84)|20244|\n",
    "|RELU Activation|(None, 84)|0|\n",
    "|Dropout|(None, 84)|0|\n",
    "|**Final fully Connnected Layer3**|(None, 43)|3655|\n",
    "\n",
    "Total params: 435,019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def LeNet_adapted(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Convolutional. Input = 32x32x3. Output = 28x28x16.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 16), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(16))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "    \n",
    "    # Activation. Input = 28x28x16. Output = 28x28x16.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    # Pooling. Input = 28x28x16. Output = 14x14x16.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Layer 2: Convolutional. Input = 14x14x16. Output = 10x10x64.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 64), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(64))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # Activation.Input = 10x10x64. Output = 10x10x64.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    # Pooling. Input = 10x10x64. Output = 5x5x64.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Flatten. Input = 5x5x64. Output = 1600.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 1600. Output = 240.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(1600, 240), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(240))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # Activation. Input = 240. Output = 240.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Dropout Input = Input = 240. Output = 240.\n",
    "    print  (keep_prob)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    # Layer 4: Fully Connected. Input = 240. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(240, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # Activation. Input = 84. Output = 84.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Dropout. Input = 84. Output = 84.\n",
    "    fc2 = tf.nn.dropout (fc2, keep_prob)\n",
    "    \n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits, conv1, conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate  and Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x` is a placeholder for a batch of input images.<br>\n",
    "`y` is a placeholder for a batch of output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline\n",
    "Create a training pipeline that uses the model to classify Traffic sign data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "logits, conv1, conv2 = LeNet_adapted(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "if perform_training:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        num_examples = len(X_train)\n",
    "        validation_accuracy_list = []\n",
    "        print('Training...')\n",
    "        print ()\n",
    "        for i in range(EPOCHS):\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "            for offset in range(0, num_examples, BATCH_SIZE):\n",
    "                end = offset + BATCH_SIZE\n",
    "                batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "                sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.6})\n",
    "\n",
    "            validation_accuracy = evaluate(X_valid, y_valid)\n",
    "            validation_accuracy_list.append (validation_accuracy)\n",
    "            print('EPOCH {:d} \\t Validation Accuracy = {:.3f} '.format(i+1 , validation_accuracy))\n",
    "\n",
    "        training_accuracy = evaluate (X_train, y_train)  \n",
    "        print('Accuracy: Train: {:.3f}, Validation: {:.3f}'.format(training_accuracy, validation_accuracy))\n",
    "        saver.save(sess, model_path )\n",
    "        print(\"Model saved\")\n",
    "else:\n",
    "    print ('Training not performed. Set flag perform_training to True.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training key performance indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_training:\n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(5, 4))\n",
    "    ax.plot(range(1, EPOCHS + 1), validation_accuracy_list)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Validation Accuracy')\n",
    "    ax.grid(True)\n",
    "else:\n",
    "    print ('Training not performed. No statistics displayed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_training:\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        test_accuracy = evaluate(X_test, y_test)\n",
    "        print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "else:\n",
    "    print ('Training not performed. No statistics possible.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "folder = './test_images/batch1/*.jpg'\n",
    "#In the above folder, the traffic signs should be saved as '<class_id>jpg'\n",
    "#the class-ids are available in ./signnames.csv\n",
    "#web_files would be used in analysis of results\n",
    "web_files = glob.glob(folder)\n",
    "X_test_web = [ mpimg.imread('./' + image_file ) for image_file in web_files ]\n",
    "#scale to it 32x32\n",
    "X_test_web = [ cv2.resize(image, (32,32)) for image in X_test_web ]\n",
    "#Plot the source images\n",
    "fig, axes = plt.subplots(ncols=len(X_test_web), figsize=(24, 12))\n",
    "for ax, image in zip(axes, X_test_web):\n",
    "    ax.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-prcoessing the web-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-prcoessing the web-images\n",
    "import numpy as np\n",
    "#same function which was used for train, valid and test datasets\n",
    "#copied to enable independent execution of Step 3\n",
    "def normalize_webimages(images):\n",
    "    result = np.zeros((len(images), 32, 32, 3), np.float64)\n",
    "    for index, image in enumerate(images):\n",
    "        gray = cv2.cvtColor(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "        result[index] = np.array((gray - np.min(gray)) / (np.max(gray) - np.min(gray)))\n",
    "    return result\n",
    "\n",
    "X_test_web = normalize_webimages(X_test_web)\n",
    "#Plot the normalized\n",
    "fig, axes = plt.subplots(ncols=len(X_test_web), figsize=(24, 12))\n",
    "for ax, image in zip(axes, X_test_web):\n",
    "    ax.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## https://stackoverflow.com/questions/33181846/programmatically-convert-pandas-dataframe-to-markdown-table\n",
    "def pandas_df_to_markdown_table(df):\n",
    "    from IPython.display import Markdown, display\n",
    "    fmt = ['---' for i in range(len(df.columns))]\n",
    "    df_fmt = pd.DataFrame([fmt], columns=df.columns)\n",
    "    df_formatted = pd.concat([df_fmt, df])\n",
    "    display(Markdown(df_formatted.to_csv(sep=\"|\", index=False)))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    #restore the models from the model_path folder\n",
    "    saver.restore(sess, model_path)\n",
    "    result_classes = sess.run(logits, feed_dict={x: X_test_web, keep_prob : 1.0})\n",
    "    result_softmax = sess.run(tf.nn.softmax(logits), feed_dict={x: X_test_web, keep_prob : 1.0})\n",
    "    result_predict = sess.run(tf.nn.top_k(result_softmax, k=5, sorted=True))\n",
    "    name_predictions = np.array([label_texts[result_predict[1][i][0]] for i in range(0, len(web_files))])\n",
    "\n",
    "    #derive the target id's from the image files name\n",
    "    target_class_ids = np.array([ os.path.splitext(os.path.basename(image_path))[0] for image_path in web_files ])\n",
    "    #get the corresponding description from the label_texts array, which was loaded from the signnames.csv\n",
    "    target_class_labels = np.array([label_texts[int(target_class_ids[i])] for i in range(0, len(web_files))])\n",
    "    \n",
    "    #prepare the results and show in a table\n",
    "    data = np.stack((name_predictions, target_class_labels), axis=1)\n",
    "    df = pd.DataFrame(data, target_class_ids, ['Predictions' , 'Target'])\n",
    "    pandas_df_to_markdown_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, 4 out of 5 images were correctly predicted. This gives a result of 80% accuracy in the test performance.\n",
    "<TODO>: Deep-dive on the root cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the accuracy for these 5 new images. \n",
    "### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images.\n",
    "cols = len(result_classes)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=cols, figsize=(4*cols, 4))\n",
    "for ax, one_class, title in zip(axes, result_classes, name_predictions):        \n",
    "    ax.bar(range(0,43), one_class)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "folder = './test_images/batch1/*.jpg'\n",
    "web_files = glob.glob(folder)\n",
    "X_test_web = [ mpimg.imread('./' + image_file ) for image_file in web_files ]\n",
    "X_test_web = [ cv2.resize(image, (32,32)) for image in X_test_web ]\n",
    "fig, axes = plt.subplots(ncols=len(X_test_web), figsize=(20,12))\n",
    "for ax, image in zip(axes, X_test_web):\n",
    "    ax.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the new images, print out the model's softmax probabilities to show the **certainty** of the model's predictions (limit the output to the top 5 probabilities for each image). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(ncols=2, nrows = 5, figsize=(24, 8))\n",
    "#for ax, classname, filename in zip(axes, name_predictions, result_predict):        \n",
    "for i in range(len(result_predict[0])):\n",
    "    print('Probabilities:')\n",
    "    for j in range(0, len(result_predict[0][i])):\n",
    "        prob = result_predict[0][i][j]\n",
    "        index = result_predict[1][i][j]\n",
    "        name = label_texts[index]\n",
    "        print('   {:.6f} : {} - {}'.format(prob, index, name))\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Writeup\n",
    "The project writeup can be found here: [link]((https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4 : Visualize the Neural Network's State with Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize your network's feature maps here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    if featuremaps > 48:\n",
    "        featuremaps = 48\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, \n",
    "                       vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"conv1 : First layer\")\n",
    "    outputFeatureMap(X_test_web, conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_path)\n",
    "    \n",
    "    web_classes = sess.run(logits, feed_dict={x: X_test_web, keep_prob : 1.0})\n",
    "    print(\"conv2 : Second layer\")\n",
    "    outputFeatureMap(X_test_web, conv2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
